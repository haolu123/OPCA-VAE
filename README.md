# PCA-VAE: Differentiable Quantization via Online PCA (Ojaâ€™s Rule)

> A fully-differentiable alternative to vector quantization with orthogonal, interpretable, and bit-efficient latents.

âœ… No straight-through estimator  
âœ… No codebook collapse  
âœ… Orthogonal + variance-ordered latent axes  
âœ… 10Ã—â€“100Ã— higher bit-efficiency vs VQ  
âœ… Strong reconstructions + interpretable factors  

---

<p align="center" style="background:white;padding:10px;">
  <img src="./figures/PCA_vae_scheme.png" width="80%">
</p>

**PCA-VAE replaces VQ with an online PCA layer**, learned via **Ojaâ€™s rule**, inside a VAE bottleneck.  
This yields a continuous, stable, and interpretable latent space without discrete lookup tables.

---

## âœ¨ Highlights

| Feature | PCA-VAE | VQ-VAE / VQGAN |
|--------|--------|----------------|
| Differentiable | âœ… Yes | âŒ Needs STE |
| Codebook collapse | âŒ None | âš ï¸ Common |
| Training stability | âœ… High | âš ï¸ Sensitive |
| Latent semantics | âœ… Ordered + axis-aligned | â“ Emergent |
| Bit-efficiency | âœ… 10Ã—â€“100Ã— better | âŒ Worse |
| Implementation | ğŸŸ¢ Simple | ğŸŸ¡ Codebooks / EMA |

---

## ğŸ“‚ Repository Structure

```
OPCA-VAE/            # Training code & PCA module (Ojaâ€™s rule)
â”‚â”€â”€ models/          # Encoder/Decoder/PCA layer
â”‚â”€â”€ dataset.py
â”‚â”€â”€ experiment.py
â”‚â”€â”€ configs_rebuild/
â”‚â”€â”€ run.py/
â”‚â”€â”€ pca_poke_tools.py
figures/
â”‚â”€â”€ PCA_vae_scheme.png   # Architecture
â”‚â”€â”€ radar.png            # Multi-metric radar comparison
â”‚â”€â”€ budget_curve_rfid.png# Bit-budget curve (example)
â”‚â”€â”€ scan_c3_strip.png    # Latent traversal (c0â€“c5)
â”‚â”€â”€ scan_c9_11_strip.png # Latent traversal (c8 & c10)
```

---

## ğŸš€ Install & Train

See OPCAE-VAE folder for details.

## ğŸ“Š Key Results

### Multi-metric reconstruction comparison

<p align="center"><img src="./figures//radar.png" width="55%"></p>

**Interpretation:** PCA-VAE (16Ã—16) forms the largest radar polygon â†’ best overall across  
PSNR â†‘, SSIM â†‘ are used min-max normalization; LPIPS â†“, rFID â†“ are used 1-min-max normalization. So that 1 is always the best.  
Baselines: VQGAN_FC-16Ã—16, SimVQ-16Ã—16, VQ-VAE v2, AutoencoderKL.

---

### Bit-budget efficiency

<p align="center"><img src="./figures//budget_curve_rfid.png" width="70%"></p>

**Same latent bit budget** â†’ PCA-VAE achieves **lower rFID** than VQ-based methods.  
Often matches/exceeds VQ models using **10Ã—â€“100Ã— fewer bits**.

---

### Latent semantics (interpretable axes)

<p align="center"><img src="./figures//scan_c3_strip.png" width="80%"></p>
<p align="center"><img src="./figures//scan_c9_11_strip.png" width="80%"></p>

Each latent dimension corresponds to a **meaningful change**:
lighting, pose, softness/gender-like cues, sunglasses, hair volume, etc.  
Dimensions are **naturally ordered by explained variance**.

---

## ğŸ§  Concept

PCA layer forward/backward:

$$
Y = C^T(h - \mu), \quad \hat{h} = CY + \mu
$$

- `C`: orthogonal basis learned via **Ojaâ€™s rule** (online PCA)  
- Latents are **orthogonal and variance-sorted**  
- Entire pipeline is **fully differentiable**

No codebooks. No STE. Pure gradients.

---


## ğŸ“¬ Contact

Questions / suggestions â€” please open an issue or contact: `Hao.Lu@advocatehealth.org`
